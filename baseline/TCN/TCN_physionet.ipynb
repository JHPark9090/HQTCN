{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3567ec6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pennylane Version : 0.41.1\n",
      "Pytorch Version : 2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Optional, Tuple, Callable\n",
    "import mne\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "print('Pennylane Version :', qml.__version__)\n",
    "print('Pytorch Version :', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6b41cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"Running on \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987abe90",
   "metadata": {},
   "source": [
    "# Prepare PhysioNet EEG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2703183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg_ts(seed, device, batch_size, sampling_freq):\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        \n",
    "    # Load and preprocess the PhysioNet EEG Motor Imagery data\n",
    "    N_SUBJECT = 50\n",
    "    IMAGINE_OPEN_CLOSE_LEFT_RIGHT_FIST = [4, 8, 12]\n",
    "\n",
    "    # Load data from PhysioNet (example assumes data is downloaded locally)\n",
    "    physionet_paths = [\n",
    "        mne.datasets.eegbci.load_data(\n",
    "            subjects=subj_id,\n",
    "            runs=IMAGINE_OPEN_CLOSE_LEFT_RIGHT_FIST,\n",
    "            path=\"PhysioNet_EEG\",\n",
    "        ) for subj_id in range(1, N_SUBJECT+1)\n",
    "    ]\n",
    "    physionet_paths = np.concatenate(physionet_paths)\n",
    "\n",
    "    # Ensuring that all subjects share same sampling frequency\n",
    "    # TARGET_SFREQ = 160  # 160 Hz sampling rate\n",
    "    TARGET_SFREQ = sampling_freq\n",
    "\n",
    "    # Concatenate all loaded raw data\n",
    "    parts = []\n",
    "    for path in physionet_paths:\n",
    "        raw = mne.io.read_raw_edf(\n",
    "            path,\n",
    "            preload=True,\n",
    "            stim_channel='auto',\n",
    "            verbose='WARNING',\n",
    "        )\n",
    "        # Resample raw data to ensure consistent sfreq\n",
    "        raw.resample(TARGET_SFREQ, npad=\"auto\")\n",
    "        parts.append(raw)\n",
    "        \n",
    "    # Concatenate resampled raw data\n",
    "    raw = mne.concatenate_raws(parts)\n",
    "\n",
    "    # Pick EEG channels and extract events\n",
    "    eeg_channel_inds = mne.pick_types(\n",
    "        raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads'\n",
    "    )\n",
    "    events, _ = mne.events_from_annotations(raw)\n",
    "\n",
    "    # Epoch the data\n",
    "    epoched = mne.Epochs(\n",
    "        raw, events, dict(left=2, right=3), tmin=1, tmax=4.1,\n",
    "        proj=False, picks=eeg_channel_inds, baseline=None, preload=True\n",
    "    )\n",
    "\n",
    "    # Convert data to NumPy arrays\n",
    "    X = (epoched.get_data() * 1e3).astype(np.float32)  # Convert to millivolts\n",
    "    y = (epoched.events[:, 2] - 2).astype(np.int64)  # 0: left, 1: right\n",
    "    \n",
    "    # Train-validation-test split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    def MakeTensorDataset(X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        tensordataset = TensorDataset(X_tensor, y_tensor)\n",
    "        return tensordataset\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = MakeTensorDataset(X_train, y_train)\n",
    "    val_dataset = MakeTensorDataset(X_val, y_val)\n",
    "    test_dataset = MakeTensorDataset(X_test, y_test)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    input_dim = X_train.shape\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d9e0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNE Version : 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import mne\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('MNE Version :', mne.__version__)\n",
    "\n",
    "def load_eeg_ts_revised(seed, device, batch_size, sampling_freq, sample_size):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the PhysioNet EEG Motor Imagery dataset for a specified number of subjects.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        device (torch.device): The device to move the tensors to.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        sampling_freq (int): The target sampling frequency to resample the data to.\n",
    "        sample_size (int): The number of subjects to load data from (1 to 109).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (train_loader, test_loader, input_dim).\n",
    "               - train_loader: DataLoader for the training set.\n",
    "               - val_loader: DataLoader for the validation set.\n",
    "               - test_loader: DataLoader for the test set.\n",
    "               - input_dim: The shape of the input data (n_trials, n_channels, n_timesteps).\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1: Split Subject IDs into Train, Validation, and Test Sets ---\n",
    "    N_SUBJECT = sample_size\n",
    "    subject_ids = np.arange(1, N_SUBJECT + 1)\n",
    "    \n",
    "    # Split subjects: ~70% train, ~15% validation, ~15% test\n",
    "    train_subjects, temp_subjects = train_test_split(\n",
    "        subject_ids,\n",
    "        test_size=0.3, # 30% for validation and test combined\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    val_subjects, test_subjects = train_test_split(\n",
    "        temp_subjects,\n",
    "        test_size=0.5, # Split the 30% evenly into 15% validation and 15% test\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    print(f\"Subjects in Training Set: {len(train_subjects)}\")\n",
    "    print(f\"Subjects in Validation Set: {len(val_subjects)}\")\n",
    "    print(f\"Subjects in Test Set: {len(test_subjects)}\")\n",
    "\n",
    "\n",
    "    # ⬇️ 추가: 보기 좋게 정렬 + 겹침 여부 검증 + 화면/파일 출력\n",
    "    train_subjects_sort = np.sort(train_subjects)\n",
    "    val_subjects_sort   = np.sort(val_subjects)\n",
    "    test_subjects_sort  = np.sort(test_subjects)\n",
    "\n",
    "    # 겹치는지 안전 체크\n",
    "    assert not set(train_subjects_sort) & set(val_subjects_sort)\n",
    "    assert not set(train_subjects_sort) & set(test_subjects_sort)\n",
    "    assert not set(val_subjects_sort)   & set(test_subjects_sort)\n",
    "\n",
    "    print(f\"Train subjects ({len(train_subjects_sort)}): {train_subjects_sort.tolist()}\")\n",
    "    print(f\"Val subjects   ({len(val_subjects_sort)}): {val_subjects_sort.tolist()}\")\n",
    "    print(f\"Test subjects  ({len(test_subjects_sort)}): {test_subjects_sort.tolist()}\")\n",
    "\n",
    "    # 필요하면 텍스트 파일로도 남기기 (옵션)\n",
    "    np.savetxt(f\"results/tcn{seed}_{sample_size}samples_train_subjects.txt\", train_subjects_sort, fmt=\"%d\")\n",
    "    np.savetxt(f\"results/tcn{seed}_{sample_size}samples_val_subjects.txt\",   val_subjects_sort,   fmt=\"%d\")\n",
    "    np.savetxt(f\"results/tcn{seed}_{sample_size}samples_test_subjects.txt\",  test_subjects_sort,  fmt=\"%d\")\n",
    "    \n",
    "\n",
    "    # --- Step 2: Define a Helper Function to Load Data for a Given List of Subjects ---\n",
    "    def _load_and_process_subjects(subject_list, sfreq):\n",
    "        IMAGINE_OPEN_CLOSE_LEFT_RIGHT_FIST = [4, 8, 12]\n",
    "        \n",
    "        # Load file paths for the specified subjects\n",
    "        physionet_paths = [\n",
    "            mne.datasets.eegbci.load_data(\n",
    "                subjects=subj_id,\n",
    "                runs=IMAGINE_OPEN_CLOSE_LEFT_RIGHT_FIST,\n",
    "                path=\"PhysioNet_EEG\",\n",
    "            ) for subj_id in subject_list\n",
    "        ]\n",
    "        physionet_paths = np.concatenate(physionet_paths)\n",
    "\n",
    "        parts = []\n",
    "        for path in physionet_paths:\n",
    "            raw = mne.io.read_raw_edf(\n",
    "                path, preload=True, stim_channel='auto', verbose='WARNING'\n",
    "            )\n",
    "            raw.resample(sfreq, npad=\"auto\")\n",
    "            parts.append(raw)\n",
    "        \n",
    "        # Concatenate all runs FOR THIS SPLIT into one raw object\n",
    "        raw = mne.concatenate_raws(parts)\n",
    "\n",
    "        # Epoch the data\n",
    "        events, _ = mne.events_from_annotations(raw)\n",
    "        eeg_channel_inds = mne.pick_types(\n",
    "            raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads'\n",
    "        )\n",
    "        epoched = mne.Epochs(\n",
    "            raw, events, dict(left=2, right=3), tmin=1, tmax=4.1,\n",
    "            proj=False, picks=eeg_channel_inds, baseline=None, preload=True\n",
    "        )\n",
    "\n",
    "        # Convert to NumPy arrays\n",
    "        X = (epoched.get_data() * 1e3).astype(np.float32)\n",
    "        y = (epoched.events[:, 2] - 2).astype(np.int64)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    # --- Step 3: Load Data Separately for Each Subject Group ---\n",
    "    X_train, y_train = _load_and_process_subjects(train_subjects, sampling_freq)\n",
    "    X_val, y_val = _load_and_process_subjects(val_subjects, sampling_freq)\n",
    "    X_test, y_test = _load_and_process_subjects(test_subjects, sampling_freq)\n",
    "    \n",
    "    print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "    # --- Step 4: Create PyTorch Datasets and DataLoaders ---\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device),\n",
    "                                  torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device),\n",
    "                                torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device),\n",
    "                                 torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    input_dim = X_train.shape\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8baf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects in Training Set: 7\n",
      "Subjects in Validation Set: 1\n",
      "Subjects in Test Set: 2\n",
      "Train subjects (7): [1, 4, 6, 7, 8, 9, 10]\n",
      "Val subjects   (1): [3]\n",
      "Test subjects  (2): [2, 5]\n",
      "Used Annotations descriptions: [np.str_('T0'), np.str_('T1'), np.str_('T2')]\n",
      "Not setting metadata\n",
      "315 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 315 events and 13 original time points ...\n",
      "15 bad epochs dropped\n",
      "Used Annotations descriptions: [np.str_('T0'), np.str_('T1'), np.str_('T2')]\n",
      "Not setting metadata\n",
      "45 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 45 events and 13 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: [np.str_('T0'), np.str_('T1'), np.str_('T2')]\n",
      "Not setting metadata\n",
      "90 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 90 events and 13 original time points ...\n",
      "6 bad epochs dropped\n",
      "\n",
      "Training set shape: (300, 64, 13)\n",
      "Validation set shape: (45, 64, 13)\n",
      "Test set shape: (84, 64, 13)\n"
     ]
    }
   ],
   "source": [
    "SEED = 2025\n",
    "SAMPLE_SIZE = 10\n",
    "\n",
    "train_loader, val_loader, test_loader, input_dim = load_eeg_ts_revised(seed=SEED, device=device, batch_size=32, sampling_freq=4, sample_size=SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4e4bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.0143,  0.1387, -0.1181,  ..., -0.0439, -0.0123, -0.0770],\n",
       "          [-0.0423,  0.1146, -0.1215,  ...,  0.0015,  0.0134, -0.0489],\n",
       "          [-0.0365,  0.1278, -0.1096,  ...,  0.0033,  0.0209, -0.0387],\n",
       "          ...,\n",
       "          [-0.0422,  0.0359, -0.0298,  ..., -0.0088,  0.0111, -0.0122],\n",
       "          [-0.0908,  0.0568,  0.0224,  ..., -0.0119,  0.0121, -0.0338],\n",
       "          [-0.0653, -0.0059, -0.0178,  ...,  0.0064, -0.0029, -0.0376]],\n",
       " \n",
       "         [[-0.0480,  0.1008, -0.0787,  ...,  0.0236,  0.0382,  0.1542],\n",
       "          [-0.0689,  0.0874, -0.1231,  ...,  0.0074,  0.0280,  0.1289],\n",
       "          [-0.0607,  0.1121, -0.1092,  ...,  0.0666,  0.0698,  0.1549],\n",
       "          ...,\n",
       "          [ 0.0032,  0.0220, -0.0426,  ...,  0.0947,  0.1242,  0.1084],\n",
       "          [ 0.0081,  0.0227, -0.0286,  ...,  0.0756,  0.0871,  0.0657],\n",
       "          [-0.0154, -0.0076, -0.0498,  ...,  0.0600,  0.0907,  0.0680]],\n",
       " \n",
       "         [[-0.0475, -0.0014,  0.0400,  ..., -0.0095,  0.1064,  0.0189],\n",
       "          [-0.0538, -0.0128,  0.0314,  ..., -0.0282,  0.0600,  0.0065],\n",
       "          [-0.0458,  0.0069,  0.0693,  ..., -0.0170,  0.0593,  0.0206],\n",
       "          ...,\n",
       "          [-0.1341, -0.0919, -0.0833,  ...,  0.0036, -0.0043,  0.0071],\n",
       "          [ 0.0038,  0.0249,  0.0421,  ...,  0.0102, -0.0106, -0.0224],\n",
       "          [-0.0115,  0.0076,  0.0111,  ...,  0.0317,  0.0023, -0.0033]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0772, -0.0838, -0.0801,  ...,  0.0273,  0.0354, -0.0083],\n",
       "          [ 0.0541, -0.0752, -0.0758,  ...,  0.0332,  0.0494, -0.0121],\n",
       "          [ 0.0468, -0.0643, -0.0731,  ...,  0.0218,  0.0457, -0.0164],\n",
       "          ...,\n",
       "          [ 0.0228, -0.0120, -0.0141,  ..., -0.0054,  0.0159, -0.0095],\n",
       "          [-0.0003, -0.0192, -0.0304,  ...,  0.0423,  0.0412, -0.0098],\n",
       "          [ 0.0120, -0.0086, -0.0045,  ...,  0.0171,  0.0153, -0.0060]],\n",
       " \n",
       "         [[-0.0811, -0.0444, -0.0503,  ...,  0.0255,  0.0265, -0.0048],\n",
       "          [-0.0826, -0.0342, -0.0272,  ...,  0.0418,  0.0518, -0.0033],\n",
       "          [-0.0936, -0.0500, -0.0463,  ...,  0.0312,  0.0446, -0.0019],\n",
       "          ...,\n",
       "          [-0.0261,  0.0065,  0.0166,  ...,  0.0160,  0.0292, -0.0053],\n",
       "          [-0.0107,  0.0200,  0.0151,  ..., -0.0018,  0.0166, -0.0102],\n",
       "          [ 0.0077,  0.0173,  0.0158,  ..., -0.0008,  0.0209, -0.0077]],\n",
       " \n",
       "         [[-0.0243, -0.0021, -0.0388,  ...,  0.0115,  0.0194,  0.0819],\n",
       "          [-0.0561, -0.0376, -0.0472,  ..., -0.0008,  0.0072,  0.0641],\n",
       "          [-0.0195, -0.0007, -0.0044,  ..., -0.0037, -0.0031,  0.0768],\n",
       "          ...,\n",
       "          [ 0.0726,  0.0487, -0.0024,  ...,  0.0072,  0.0196,  0.0126],\n",
       "          [ 0.0788,  0.0711,  0.0326,  ..., -0.0174, -0.0092, -0.0123],\n",
       "          [ 0.0747,  0.0736,  0.0403,  ...,  0.0069,  0.0089,  0.0101]]],\n",
       "        device='cuda:0'),\n",
       " tensor([0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467a7976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1492, 64, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929b01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TemporalConvNet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGTCNClassifier(nn.Module):\n",
    "    def __init__(self, in_ch, tcn_ch, num_classes, k=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.tcn = TemporalConvNet(in_ch, tcn_ch, k, dropout)\n",
    "        self.linear = nn.Linear(tcn_ch[-1], num_classes)\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        y = self.tcn(x)            # → (B, C_out, L)\n",
    "        logit = self.linear(y[:, :, -1])\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f80bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Calculate Running Time ########################################\n",
    "def epoch_time(start_time: float, end_time: float) -> Tuple[float, float]:\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "################################# Performance & Density Matrices ################################\n",
    "# Training loop\n",
    "def train_perf(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        assert not torch.isnan(inputs).any(), \"Inputs contain NaN!\"\n",
    "        assert not torch.isinf(inputs).any(), \"Inputs contain Inf!\"\n",
    "        assert not torch.isnan(labels).any(), \"Labels contain NaN!\"\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Ensure that data is on the same device (GPU or CPU)\n",
    "        labels = labels.float()   # Ensure labels are of type float for BCEWithLogitsLoss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Collect labels and outputs for AUROC\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_outputs.append(outputs.detach().cpu().numpy())       \n",
    "    # Calculate train AUROC\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    print(\"labels NaNs:\", np.isnan(all_labels).sum(), \"/\", all_labels.shape[0])\n",
    "    print(\"outputs NaNs:\", np.isnan(all_outputs).sum(), \"/\", all_outputs.shape[0])\n",
    "    train_auroc = roc_auc_score(all_labels, all_outputs)\n",
    "    \n",
    "    return train_loss / len(dataloader), train_auroc\n",
    "\n",
    "\n",
    "# Validation/Test loop\n",
    "def evaluate_perf(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Ensure that data is on the same device (GPU or CPU)\n",
    "            labels = labels.float()   # Ensure labels are of type float for BCEWithLogitsLoss\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Collect labels and outputs for AUROC\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    auroc = roc_auc_score(all_labels, all_outputs)\n",
    "    \n",
    "    return running_loss / len(dataloader), auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f51a4532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects in Training Set: 35\n",
      "Subjects in Validation Set: 7\n",
      "Subjects in Test Set: 8\n",
      "Train subjects (35): [1, 3, 4, 5, 8, 9, 10, 12, 13, 14, 15, 17, 18, 20, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 43, 44, 45, 46, 47, 48]\n",
      "Val subjects   (7): [6, 11, 24, 26, 35, 42, 49]\n",
      "Test subjects  (8): [2, 7, 16, 19, 21, 22, 40, 50]\n",
      "Used Annotations descriptions: [np.str_('T0'), np.str_('T1'), np.str_('T2')]\n",
      "Not setting metadata\n",
      "1575 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 1575 events and 13 original time points ...\n",
      "83 bad epochs dropped\n",
      "Used Annotations descriptions: [np.str_('T0'), np.str_('T1'), np.str_('T2')]\n",
      "Not setting metadata\n",
      "315 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 315 events and 13 original time points ...\n",
      "18 bad epochs dropped\n",
      "Used Annotations descriptions: [np.str_('T0'), np.str_('T1'), np.str_('T2')]\n",
      "Not setting metadata\n",
      "360 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 360 events and 13 original time points ...\n",
      "15 bad epochs dropped\n",
      "\n",
      "Training set shape: (1492, 64, 13)\n",
      "Validation set shape: (297, 64, 13)\n",
      "Test set shape: (345, 64, 13)\n"
     ]
    }
   ],
   "source": [
    "SEED = 2027\n",
    "SAMPLE_SIZE = 50\n",
    "\n",
    "train_loader, val_loader, test_loader, input_dim = load_eeg_ts_revised(seed=SEED, device=device, batch_size=32, sampling_freq=4, sample_size=SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/alveare/.conda/envs/qml/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 5,857  |  Trainable: 5,857  |  Frozen: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 89.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 396.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 0s\n",
      "Train Loss: 0.6931, AUC: 0.5150 | Validation Loss: 0.6928, AUC: 0.6823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 91.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 387.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 0s\n",
      "Train Loss: 0.6925, AUC: 0.5274 | Validation Loss: 0.6926, AUC: 0.7158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 96.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 354.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 0s\n",
      "Train Loss: 0.6912, AUC: 0.5519 | Validation Loss: 0.6920, AUC: 0.6782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 93.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 377.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 0s\n",
      "Train Loss: 0.6909, AUC: 0.5585 | Validation Loss: 0.6914, AUC: 0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 90.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 384.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 0s\n",
      "Train Loss: 0.6893, AUC: 0.5779 | Validation Loss: 0.6898, AUC: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 91.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 388.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 0s\n",
      "Train Loss: 0.6861, AUC: 0.6175 | Validation Loss: 0.6881, AUC: 0.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 92.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 373.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 0s\n",
      "Train Loss: 0.6822, AUC: 0.6527 | Validation Loss: 0.6851, AUC: 0.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 94.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 393.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 0s\n",
      "Train Loss: 0.6765, AUC: 0.6627 | Validation Loss: 0.6819, AUC: 0.7230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 92.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 379.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 0s\n",
      "Train Loss: 0.6687, AUC: 0.6902 | Validation Loss: 0.6774, AUC: 0.7295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 96.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 399.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 0s\n",
      "Train Loss: 0.6592, AUC: 0.7036 | Validation Loss: 0.6690, AUC: 0.7359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 93.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 372.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 0m 0s\n",
      "Train Loss: 0.6527, AUC: 0.7093 | Validation Loss: 0.6641, AUC: 0.7476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 92.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 363.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 0m 0s\n",
      "Train Loss: 0.6417, AUC: 0.7222 | Validation Loss: 0.6549, AUC: 0.7371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 91.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 381.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time: 0m 0s\n",
      "Train Loss: 0.6286, AUC: 0.7397 | Validation Loss: 0.6441, AUC: 0.7493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 92.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 375.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time: 0m 0s\n",
      "Train Loss: 0.6203, AUC: 0.7338 | Validation Loss: 0.6405, AUC: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 97.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 389.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time: 0m 0s\n",
      "Train Loss: 0.6162, AUC: 0.7437 | Validation Loss: 0.6337, AUC: 0.7477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 98.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 379.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Time: 0m 0s\n",
      "Train Loss: 0.6068, AUC: 0.7588 | Validation Loss: 0.6283, AUC: 0.7531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 95.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 368.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Time: 0m 0s\n",
      "Train Loss: 0.6001, AUC: 0.7632 | Validation Loss: 0.6290, AUC: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 95.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 380.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Time: 0m 0s\n",
      "Train Loss: 0.6079, AUC: 0.7508 | Validation Loss: 0.6238, AUC: 0.7512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 98.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 396.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Time: 0m 0s\n",
      "Train Loss: 0.5959, AUC: 0.7612 | Validation Loss: 0.6220, AUC: 0.7493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 97.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels NaNs: 0 / 1492\n",
      "outputs NaNs: 0 / 1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 380.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Time: 0m 0s\n",
      "Train Loss: 0.5847, AUC: 0.7791 | Validation Loss: 0.6188, AUC: 0.7505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 368.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5799, AUC: 0.8026\n",
      "Metrics saved to results/TCN_performance_2027_50samples_lr0.0005_small.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20  # Set the number of epochs for training\n",
    "\n",
    "model = EEGTCNClassifier(in_ch=64, tcn_ch=[16, 16, 16], num_classes=1, k=2, dropout=0.3).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total: {total_params:,}  |  Trainable: {trainable_params:,}  |  Frozen: {total_params-trainable_params:,}\")\n",
    "\n",
    "lr = 0.0005\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4, eps=1e-8)\n",
    "\n",
    "# Training process\n",
    "train_metrics, valid_metrics, test_metrics = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_auc = train_perf(model, train_loader, optimizer, criterion)\n",
    "    train_metrics.append({'epoch': epoch + 1, 'train_loss': train_loss, 'train_auc': train_auc})    \n",
    "\n",
    "    valid_loss, valid_auc = evaluate_perf(model, val_loader, criterion)\n",
    "    valid_metrics.append({'epoch': epoch + 1, 'valid_loss': valid_loss, 'valid_auc': valid_auc})\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f\"Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, AUC: {train_auc:.4f} | Validation Loss: {valid_loss:.4f}, AUC: {valid_auc:.4f}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "test_loss, test_auc = evaluate_perf(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, AUC: {test_auc:.4f}\")\n",
    "test_metrics.append({'epoch': num_epochs, 'test_loss': test_loss, 'test_auc': test_auc}) \n",
    "\n",
    "# Combine all metrics into a pandas DataFrame\n",
    "metrics = []\n",
    "for epoch in range(num_epochs):\n",
    "    metrics.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_metrics[epoch]['train_loss'],\n",
    "        'train_auc': train_metrics[epoch]['train_auc'],\n",
    "        'valid_loss': valid_metrics[epoch]['valid_loss'],\n",
    "        'valid_auc': valid_metrics[epoch]['valid_auc'],\n",
    "        'test_loss': test_metrics[0]['test_loss'],\n",
    "        'test_auc': test_metrics[0]['test_auc'],\n",
    "    })\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "# Save to CSV\n",
    "csv_filename = f\"results/TCN_performance_{SEED}_{SAMPLE_SIZE}samples_lr{lr}_small.csv\"\n",
    "metrics_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Metrics saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61d4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
